import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from lightning.fabric import Fabric
from model_v0 import CNNTransformerBase

from data_loader import OCRDataset, collate_fn
from train import compute_loss, decode_predictions, compute_metrics, compute_crr
from torch.optim import Adam
import argparse
import os
import glob
from datetime import datetime
from ocr_transforms import get_ocr_transforms, get_light_augmentation, get_heavy_augmentation
import numpy as np

from line_profiler import profile
torch.set_float32_matmul_precision('high')

def compute_edit_distance_details(pred_text, true_text):
    """
    è®¡ç®—ç¼–è¾‘è·ç¦»åŠæ’å…¥ã€åˆ é™¤ã€æ›¿æ¢æ“ä½œçš„æ•°é‡
    
    Args:
        pred_text: é¢„æµ‹æ–‡æœ¬
        true_text: çœŸå®æ–‡æœ¬
    
    Returns:
        edit_distance: ç¼–è¾‘è·ç¦»
        insertions: æ’å…¥æ“ä½œæ•°é‡
        deletions: åˆ é™¤æ“ä½œæ•°é‡
        substitutions: æ›¿æ¢æ“ä½œæ•°é‡
    """
    # åˆ›å»ºDPè¡¨æ ¼
    dp = np.zeros((len(true_text) + 1, len(pred_text) + 1), dtype=np.int32)
    
    # æ“ä½œç±»å‹è¡¨æ ¼: 0=æ— æ“ä½œ, 1=æ’å…¥, 2=åˆ é™¤, 3=æ›¿æ¢
    operations = np.zeros((len(true_text) + 1, len(pred_text) + 1), dtype=np.int32)
    
    # åˆå§‹åŒ–ç¬¬ä¸€è¡Œå’Œç¬¬ä¸€åˆ—
    for i in range(len(true_text) + 1):
        dp[i, 0] = i
        if i > 0:
            operations[i, 0] = 2  # åˆ é™¤
    
    for j in range(len(pred_text) + 1):
        dp[0, j] = j
        if j > 0:
            operations[0, j] = 1  # æ’å…¥
    
    # å¡«å……DPè¡¨æ ¼
    for i in range(1, len(true_text) + 1):
        for j in range(1, len(pred_text) + 1):
            if true_text[i-1] == pred_text[j-1]:
                dp[i, j] = dp[i-1, j-1]
                operations[i, j] = 0  # åŒ¹é…
            else:
                deletion = dp[i-1, j] + 1
                insertion = dp[i, j-1] + 1
                substitution = dp[i-1, j-1] + 1
                
                # æ‰¾åˆ°æœ€å°æ“ä½œ
                min_op = min(deletion, insertion, substitution)
                dp[i, j] = min_op
                
                if min_op == deletion:
                    operations[i, j] = 2  # åˆ é™¤
                elif min_op == insertion:
                    operations[i, j] = 1  # æ’å…¥
                else:
                    operations[i, j] = 3  # æ›¿æ¢
    
    # ç»Ÿè®¡å„æ“ä½œæ•°é‡
    i, j = len(true_text), len(pred_text)
    insertions, deletions, substitutions = 0, 0, 0
    
    while i > 0 or j > 0:
        if i > 0 and j > 0 and operations[i, j] == 0:  # åŒ¹é…
            i -= 1
            j -= 1
        elif j > 0 and operations[i, j] == 1:  # æ’å…¥
            insertions += 1
            j -= 1
        elif i > 0 and operations[i, j] == 2:  # åˆ é™¤
            deletions += 1
            i -= 1
        elif i > 0 and j > 0 and operations[i, j] == 3:  # æ›¿æ¢
            substitutions += 1
            i -= 1
            j -= 1
        else:
            # å¤„ç†è¾¹ç•Œæƒ…å†µ
            if i > 0:
                deletions += 1
                i -= 1
            else:
                insertions += 1
                j -= 1
    
    edit_distance = dp[len(true_text), len(pred_text)]
    return edit_distance, insertions, deletions, substitutions

def compute_error_rates(preds, targets):
    """
    è®¡ç®—æ’å…¥é”™è¯¯ç‡ã€åˆ é™¤é”™è¯¯ç‡å’Œæ›¿æ¢é”™è¯¯ç‡
    """
    total_chars = sum(len(t) for t in targets)
    total_insertions = 0
    total_deletions = 0
    total_substitutions = 0
    
    for pred, target in zip(preds, targets):
        _, ins, dels, subs = compute_edit_distance_details(pred, target)
        total_insertions += ins
        total_deletions += dels
        total_substitutions += subs
    
    insertion_error_rate = (total_insertions / total_chars) * 100 if total_chars > 0 else 0
    deletion_error_rate = (total_deletions / total_chars) * 100 if total_chars > 0 else 0
    substitution_error_rate = (total_substitutions / total_chars) * 100 if total_chars > 0 else 0
    
    return (insertion_error_rate, total_insertions, 
            deletion_error_rate, total_deletions, 
            substitution_error_rate, total_substitutions,
            total_chars)
@profile
def train_model(augmentation='heavy', epochs=100, batch_size=16, lr=0.0001):
    """
    ä½¿ç”¨Lightning Fabricè®­ç»ƒæ¨¡å‹ (16bitç²¾åº¦)
    """
    # åˆå§‹åŒ–Fabric (å¯ç”¨16bitæ··åˆç²¾åº¦)
    fabric = Fabric(accelerator="auto", precision="16-mixed")
    fabric.launch()
    
    # åŠ è½½è¯æ±‡è¡¨
    vocab_path = 'tibetan_vocab_optimized.txt'
    with open(vocab_path, 'r', encoding='utf-8') as f:
        vocab = [line.strip() for line in f if line.strip()]
    char2idx = {char: idx for idx, char in enumerate(vocab)}
    idx2char = {idx: char for idx, char in enumerate(vocab)}
    vocab_size = len(vocab)
    
    fabric.print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    fabric.print(f"æ•°æ®å¢å¼ºæ¨¡å¼: {augmentation}")
    fabric.print(f"æ··åˆç²¾åº¦è®­ç»ƒ: 16bit")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = CNNTransformerBase(vocab_size)
    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    
    # ä½¿ç”¨FabricåŒ…è£…æ¨¡å‹å’Œä¼˜åŒ–å™¨
    model, optimizer = fabric.setup(model, optimizer)
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    os.makedirs('saved_models', exist_ok=True)
    model_files = glob.glob("saved_models/best_model*.pth")
    
    if model_files:
        latest_model = max(model_files, key=os.path.getmtime)
        fabric.print(f"ğŸ” å‘ç°é¢„è®­ç»ƒæ¨¡å‹: {latest_model}")
        try:
            state_dict = fabric.load(latest_model)
            # model.load_state_dict(state_dict)
            fabric.print("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç»§ç»­è®­ç»ƒ")
        except Exception as e:
            fabric.print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            fabric.print("ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹")
    else:
        fabric.print("ğŸ“ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶")
        fabric.print("ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹")
    
    # é…ç½®æ•°æ®å¢å¼º
    transform_map = {
        'none': transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize((64, 256)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ]),
        'light': get_light_augmentation(),
        'medium': get_ocr_transforms(train=True,img_height=128,img_width=512),
        'heavy': get_heavy_augmentation()
    }
    
    train_transform = transform_map.get(augmentation, get_ocr_transforms(train=True))
    val_transform = get_ocr_transforms(train=False)
    
    # æ•°æ®åŠ è½½
    train_dataset = OCRDataset('data/train_new.json', vocab_path, transform=train_transform)
    val_dataset = OCRDataset('data/val_new.json', vocab_path, transform=val_transform)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4,persistent_workers=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2,persistent_workers=True)
    
    # ä½¿ç”¨FabricåŒ…è£…æ•°æ®åŠ è½½å™¨
    train_loader = fabric.setup_dataloaders(train_loader)
    val_loader = fabric.setup_dataloaders(val_loader)
    
    fabric.print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
    fabric.print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
    
    # è®­ç»ƒå¾ªç¯
    best_val_wrr = 0.0
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        num_batches = 0
        
        for i, (images, texts, text_lengths, raw_texts) in enumerate(train_loader):
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            logits = model(images)
            loss = compute_loss(logits, texts, text_lengths, blank=0)
            
            # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
            if torch.isnan(loss) or torch.isinf(loss):
                fabric.print(f"âš ï¸ æŸå¤±æ— æ•ˆ: {loss.item()}")
                continue
            
            # ä½¿ç”¨Fabricçš„åå‘ä¼ æ’­ (è‡ªåŠ¨å¤„ç†16bitç²¾åº¦)
            fabric.backward(loss)
            
            # æ¢¯åº¦è£å‰ª
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            if (i + 1) % 30 == 0:
                avg_loss = total_loss / num_batches
                fabric.print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}], Loss: {avg_loss:.4f}')
        
        # æ¯10ä¸ªepochå®Œæ•´éªŒè¯
        if (epoch + 1) % 10 == 0:
            val_wrr = validate_model(fabric, model, val_loader, idx2char, epoch + 1)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_wrr > best_val_wrr:
                best_val_wrr = val_wrr
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                model_filename = f'saved_models/best_model_fabric_{timestamp}_{epoch}.pth'
                fabric.save(model_filename, model.state_dict())
                fabric.print(f'ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {model_filename}, WRR: {val_wrr:.2f}%')
        else:
            avg_train_loss = total_loss / num_batches if num_batches > 0 else 0
            fabric.print(f'Epoch [{epoch + 1}/{epochs}] è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}')
    
    fabric.print("ğŸ‰ è®­ç»ƒå®Œæˆ!")

def validate_model(fabric, model, val_loader, idx2char, epoch):
    """éªŒè¯æ¨¡å‹æ€§èƒ½"""
    model.eval()
    all_preds = []
    all_true = []
    val_loss = 0
    val_batches = 0
    
    with torch.no_grad():
        for images, texts, text_lengths, raw_texts in val_loader:
            logits = model(images)
            loss = compute_loss(logits, texts, text_lengths, blank=0)
            
            val_loss += loss.item()
            val_batches += 1
            
            preds = decode_predictions(logits, idx2char, blank=0)
            all_preds.extend(preds)
            all_true.extend(raw_texts)
    
    avg_val_loss = val_loss / val_batches if val_batches > 0 else 0
    WRR, WRR_IF, correct, correct_one_error = compute_metrics(all_preds, all_true)
    CRR, total_chars, correct_chars = compute_crr(all_preds, all_true)
    CER = 100.0 - CRR  # å­—ç¬¦é”™è¯¯ç‡ = 100% - å­—ç¬¦æ­£ç¡®ç‡
    
    # è®¡ç®—æ’å…¥ã€åˆ é™¤å’Œæ›¿æ¢é”™è¯¯ç‡
    (ins_err_rate, total_ins, 
     del_err_rate, total_del, 
     sub_err_rate, total_sub,
     _) = compute_error_rates(all_preds, all_true)
    
    fabric.print(f'\nğŸ“Š Epoch {epoch} éªŒè¯ç»“æœ:')
    fabric.print(f'éªŒè¯æŸå¤±: {avg_val_loss:.4f}')
    fabric.print(f'WRR: {WRR:.2f}% ({correct}/{len(all_true)})')
    fabric.print(f'WRR_IF: {WRR_IF:.2f}% ({correct_one_error}/{len(all_true)})')
    fabric.print(f'CRR: {CRR:.2f}% ({correct_chars}/{total_chars})')
    fabric.print(f'CER: {CER:.2f}% ({total_chars-correct_chars}/{total_chars})')
    fabric.print(f'æ’å…¥é”™è¯¯ç‡: {ins_err_rate:.2f}% ({total_ins}/{total_chars})')
    fabric.print(f'åˆ é™¤é”™è¯¯ç‡: {del_err_rate:.2f}% ({total_del}/{total_chars})')
    fabric.print(f'æ›¿æ¢é”™è¯¯ç‡: {sub_err_rate:.2f}% ({total_sub}/{total_chars})')
    
    # æ˜¾ç¤ºé¢„æµ‹ç¤ºä¾‹
    fabric.print("ğŸ“ é¢„æµ‹ç¤ºä¾‹:")
    for i in range(min(3, len(all_preds))):
        fabric.print(f"  çœŸå®: '{all_true[i]}'")
        fabric.print(f"  é¢„æµ‹: '{all_preds[i]}'")
    
    return WRR

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Lightning Fabricè®­ç»ƒè—æ–‡OCRæ¨¡å‹ (16bit)')
    parser.add_argument('--augmentation', '-a', 
                       choices=['none', 'light', 'medium', 'heavy'], 
                       default='medium',
                       help='æ•°æ®å¢å¼ºå¼ºåº¦ (default: heavy)')
    parser.add_argument('--epochs', '-e', type=int, default=300,
                       help='è®­ç»ƒè½®æ•° (default: 100)')
    parser.add_argument('--batch-size', '-b', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å° (default: 16)')
    parser.add_argument('--lr', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡ (default: 0.0001)')
    
    args = parser.parse_args()
    
    print("=== Lightning Fabric è—æ–‡OCRè®­ç»ƒ ===")
    print(f"ğŸš€ æ··åˆç²¾åº¦: 16bit")
    print(f"ğŸ“ˆ æ•°æ®å¢å¼º: {args.augmentation}")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"ğŸ“Š å­¦ä¹ ç‡: {args.lr}")
    print("=" * 35)
    
    train_model(
        augmentation=args.augmentation,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr
    ) 