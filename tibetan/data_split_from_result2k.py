import json
import os
import random
from collections import defaultdict
import argparse
from pathlib import Path


def analyze_result2k_data(json_path):
    """åˆ†æresult2k.jsonçš„æ•°æ®ç»“æ„"""
    print(f"æ­£åœ¨åˆ†æ {json_path}...")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"æ€»æ ·æœ¬æ•°: {len(data)}")
    
    # åˆ†ææ•°æ®ç»“æ„
    sample = data[0]
    print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é”®: {list(sample.keys())}")
    print(f"è½¬å½•ç±»å‹: {type(sample['transcription'])}")
    
    # ç»Ÿè®¡è½¬å½•é•¿åº¦
    transcription_stats = defaultdict(int)
    total_text_regions = 0
    
    for item in data:
        if isinstance(item['transcription'], list):
            for text in item['transcription']:
                transcription_stats[len(text)] += 1
                total_text_regions += 1
        else:
            transcription_stats[len(item['transcription'])] += 1
            total_text_regions += 1
    
    print(f"æ€»æ–‡æœ¬åŒºåŸŸæ•°: {total_text_regions}")
    print(f"è½¬å½•é•¿åº¦åˆ†å¸ƒ (å‰10):")
    for length, count in sorted(transcription_stats.items())[:10]:
        print(f"  é•¿åº¦ {length}: {count} ä¸ª")
    
    return data


def convert_to_ocr_format(data, output_dir):
    """å°†result2kæ ¼å¼è½¬æ¢ä¸ºOCRè®­ç»ƒæ ¼å¼"""
    print(f"æ­£åœ¨è½¬æ¢æ•°æ®æ ¼å¼...")
    
    ocr_samples = []
    
    for item in data:
        # è·å–å›¾ç‰‡è·¯å¾„ - å»æ‰å¼€å¤´çš„è·¯å¾„åˆ†éš”ç¬¦
        image_path = item['ocr'].lstrip('/')
        
        # å¤„ç†è½¬å½•æ–‡æœ¬
        if isinstance(item['transcription'], list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªæ–‡æœ¬åŒºåŸŸåˆ›å»ºä¸€ä¸ªæ ·æœ¬
            for i, text in enumerate(item['transcription']):
                if i < len(item['bbox']):  # ç¡®ä¿æœ‰å¯¹åº”çš„bbox
                    bbox = item['bbox'][i]
                    ocr_sample = {
                        'image_path': image_path,
                        'text': text.strip(),
                        'bbox': bbox,
                        'id': f"{item['id']}_{i}",
                        'annotation_id': item['annotation_id']
                    }
                    ocr_samples.append(ocr_sample)
        else:
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªbbox
            if item['bbox']:
                bbox = item['bbox'][0]
                ocr_sample = {
                    'image_path': image_path,
                    'text': item['transcription'].strip(),
                    'bbox': bbox,
                    'id': str(item['id']),
                    'annotation_id': item['annotation_id']
                }
                ocr_samples.append(ocr_sample)
    
    print(f"è½¬æ¢åçš„OCRæ ·æœ¬æ•°: {len(ocr_samples)}")
    return ocr_samples


def split_data(samples, train_ratio=0.8, val_ratio=0.2, random_seed=42):
    """åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    print(f"æ­£åœ¨åˆ’åˆ†æ•°æ®é›† (è®­ç»ƒé›†:{train_ratio}, éªŒè¯é›†:{val_ratio})...")
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    random.seed(random_seed)
    
    # æ‰“ä¹±æ•°æ®
    shuffled_samples = samples.copy()
    random.shuffle(shuffled_samples)
    
    # è®¡ç®—åˆ’åˆ†ç‚¹
    total_samples = len(shuffled_samples)
    train_size = int(total_samples * train_ratio)
    
    # åˆ’åˆ†æ•°æ®
    train_samples = shuffled_samples[:train_size]
    val_samples = shuffled_samples[train_size:]
    
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_samples)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_samples)}")
    
    return train_samples, val_samples


def save_splits(train_samples, val_samples, output_dir):
    """ä¿å­˜åˆ’åˆ†åçš„æ•°æ®é›†"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜è®­ç»ƒé›†
    train_path = os.path.join(output_dir, 'train_new.json')
    with open(train_path, 'w', encoding='utf-8') as f:
        json.dump(train_samples, f, ensure_ascii=False, indent=2)
    print(f"è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_path}")
    
    # ä¿å­˜éªŒè¯é›†
    val_path = os.path.join(output_dir, 'val_new.json')
    with open(val_path, 'w', encoding='utf-8') as f:
        json.dump(val_samples, f, ensure_ascii=False, indent=2)
    print(f"éªŒè¯é›†å·²ä¿å­˜åˆ°: {val_path}")
    
    return train_path, val_path


def test_data_loading(train_path, val_path):
    """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
    print("\n=== æµ‹è¯•æ•°æ®åŠ è½½ ===")
    
    try:
        # æµ‹è¯•åŠ è½½è®­ç»ƒé›†
        with open(train_path, 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        
        # æµ‹è¯•åŠ è½½éªŒè¯é›†
        with open(val_path, 'r', encoding='utf-8') as f:
            val_data = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        if train_data:
            sample = train_data[0]
            required_keys = ['image_path', 'text', 'bbox', 'id']
            missing_keys = [key for key in required_keys if key not in sample]
            if missing_keys:
                print(f"âŒ è®­ç»ƒé›†æ ·æœ¬ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_keys}")
                return False
            else:
                print("âœ… è®­ç»ƒé›†æ•°æ®æ ¼å¼æ­£ç¡®")
        
        if val_data:
            sample = val_data[0]
            missing_keys = [key for key in required_keys if key not in sample]
            if missing_keys:
                print(f"âŒ éªŒè¯é›†æ ·æœ¬ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_keys}")
                return False
            else:
                print("âœ… éªŒè¯é›†æ•°æ®æ ¼å¼æ­£ç¡®")
        
        # æ˜¾ç¤ºæ ·æœ¬ç¤ºä¾‹
        print(f"\nè®­ç»ƒé›†ç¤ºä¾‹:")
        print(f"  å›¾ç‰‡è·¯å¾„: {train_data[0]['image_path']}")
        print(f"  æ–‡æœ¬: '{train_data[0]['text']}'")
        print(f"  æ–‡æœ¬é•¿åº¦: {len(train_data[0]['text'])}")
        print(f"  è¾¹ç•Œæ¡†: {train_data[0]['bbox']}")
        
        print(f"\néªŒè¯é›†ç¤ºä¾‹:")
        print(f"  å›¾ç‰‡è·¯å¾„: {val_data[0]['image_path']}")
        print(f"  æ–‡æœ¬: '{val_data[0]['text']}'")
        print(f"  æ–‡æœ¬é•¿åº¦: {len(val_data[0]['text'])}")
        print(f"  è¾¹ç•Œæ¡†: {val_data[0]['bbox']}")
        
        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        train_lengths = [len(sample['text']) for sample in train_data]
        val_lengths = [len(sample['text']) for sample in val_data]
        
        print(f"\næ•°æ®åˆ†å¸ƒç»Ÿè®¡:")
        print(f"è®­ç»ƒé›†æ–‡æœ¬é•¿åº¦: æœ€å°={min(train_lengths)}, æœ€å¤§={max(train_lengths)}, å¹³å‡={sum(train_lengths)/len(train_lengths):.2f}")
        print(f"éªŒè¯é›†æ–‡æœ¬é•¿åº¦: æœ€å°={min(val_lengths)}, æœ€å¤§={max(val_lengths)}, å¹³å‡={sum(val_lengths)/len(val_lengths):.2f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
        train_ids = set(sample['id'] for sample in train_data)
        val_ids = set(sample['id'] for sample in val_data)
        overlap = train_ids.intersection(val_ids)
        
        if overlap:
            print(f"âš ï¸ è®­ç»ƒé›†å’ŒéªŒè¯é›†æœ‰é‡å¤ID: {len(overlap)} ä¸ª")
        else:
            print("âœ… è®­ç»ƒé›†å’ŒéªŒè¯é›†æ— é‡å¤")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


def main():
    parser = argparse.ArgumentParser(description='ä»result2k.jsoné‡æ–°åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†')
    parser.add_argument('--input', '-i', default='data/result2k.json', 
                       help='è¾“å…¥çš„result2k.jsonæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', default='data', 
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--train-ratio', type=float, default=0.9, 
                       help='è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.8)')
    parser.add_argument('--val-ratio', type=float, default=0.1, 
                       help='éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    parser.add_argument('--seed', type=int, default=42, 
                       help='éšæœºç§å­ (é»˜è®¤: 42)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¯”ä¾‹æ˜¯å¦åˆç†
    if abs(args.train_ratio + args.val_ratio - 1.0) > 0.001:
        print(f"é”™è¯¯: è®­ç»ƒé›†å’ŒéªŒè¯é›†æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º{args.train_ratio + args.val_ratio}")
        return
    
    print("=== ä»result2k.jsoné‡æ–°åˆ’åˆ†æ•°æ®é›† ===")
    print(f"è¾“å…¥æ–‡ä»¶: {args.input}")
    print(f"è¾“å‡ºç›®å½•: {args.output}")
    print(f"è®­ç»ƒé›†æ¯”ä¾‹: {args.train_ratio}")
    print(f"éªŒè¯é›†æ¯”ä¾‹: {args.val_ratio}")
    print(f"éšæœºç§å­: {args.seed}")
    print("=" * 50)
    
    # 1. åˆ†æåŸå§‹æ•°æ®
    data = analyze_result2k_data(args.input)
    
    # 2. è½¬æ¢æ•°æ®æ ¼å¼
    ocr_samples = convert_to_ocr_format(data, args.output)
    
    # 3. åˆ’åˆ†æ•°æ®é›†
    train_samples, val_samples = split_data(
        ocr_samples, 
        train_ratio=args.train_ratio, 
        val_ratio=args.val_ratio, 
        random_seed=args.seed
    )
    
    # 4. ä¿å­˜æ•°æ®é›†
    train_path, val_path = save_splits(train_samples, val_samples, args.output)
    
    # 5. æµ‹è¯•æ•°æ®åŠ è½½
    success = test_data_loading(train_path, val_path)
    
    if success:
        print("\nğŸ‰ æ•°æ®é›†åˆ’åˆ†å’Œæµ‹è¯•å®Œæˆ!")
        print(f"æ–°çš„è®­ç»ƒé›†: {train_path}")
        print(f"æ–°çš„éªŒè¯é›†: {val_path}")
    else:
        print("\nâŒ æ•°æ®é›†åˆ’åˆ†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")


if __name__ == '__main__':
    main() 