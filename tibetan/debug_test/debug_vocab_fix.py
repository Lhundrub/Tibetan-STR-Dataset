import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from model import CNNTransformer
from data_loader import OCRDataset, collate_fn
from train import compute_loss

def debug_vocab_fix():
    """éªŒè¯è¯æ±‡è¡¨ä¿®å¤"""
    print("=== è¯æ±‡è¡¨ä¿®å¤éªŒè¯ ===")
    
    # ä½¿ç”¨ä¼˜åŒ–è¯æ±‡è¡¨
    vocab_path = 'tibetan_vocab_optimized.txt'
    
    # åŠ è½½è¯æ±‡è¡¨
    with open(vocab_path, 'r', encoding='utf-8') as f:
        vocab = [line.strip() for line in f if line.strip()]
    char2idx = {char: idx for idx, char in enumerate(vocab)}
    idx2char = {idx: char for idx, char in enumerate(vocab)}
    vocab_size = len(vocab)
    
    print(f"ğŸ“š è¯æ±‡è¡¨ä¿¡æ¯:")
    print(f"  æ–‡ä»¶: {vocab_path}")
    print(f"  å¤§å°: {vocab_size}")
    print(f"  å‰10ä¸ªå­—ç¬¦: {vocab[:10]}")
    print(f"  blankç´¢å¼•: {char2idx['<pad>']}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = CNNTransformer(vocab_size).to(device)
    
    print(f"\nğŸ¤– æ¨¡å‹ä¿¡æ¯:")
    print(f"  è®¾å¤‡: {device}")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    
    # æ£€æŸ¥æ¨¡å‹è¾“å‡ºå±‚
    if hasattr(model, 'classifier'):
        print(f"  æ¨¡å‹è¾“å‡ºå±‚å¤§å°: {model.classifier.out_features}")
    
    # æ•°æ®åŠ è½½å™¨æµ‹è¯•
    transform = transforms.Compose([
        transforms.Grayscale(num_output_channels=1),
        transforms.Resize((32, 128)),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])
    
    print(f"\nğŸ“Š æ•°æ®åŠ è½½å™¨æµ‹è¯•:")
    try:
        # ä½¿ç”¨å°æ ·æœ¬æµ‹è¯•
        val_dataset = OCRDataset('data/val.json', vocab_path, transform=transform)
        val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)
        
        print(f"  æ•°æ®é›†å¤§å°: {len(val_dataset)}")
        print(f"  æ•°æ®é›†è¯æ±‡è¡¨å¤§å°: {len(val_dataset.char2idx)}")
        
        # è¯æ±‡è¡¨ä¸€è‡´æ€§æ£€æŸ¥
        if vocab_size == len(val_dataset.char2idx):
            print("  âœ… è¯æ±‡è¡¨å¤§å°ä¸€è‡´")
        else:
            print(f"  âŒ è¯æ±‡è¡¨å¤§å°ä¸ä¸€è‡´: æ¨¡å‹({vocab_size}) vs æ•°æ®({len(val_dataset.char2idx)})")
            return False
        
        # æµ‹è¯•ä¸€ä¸ªbatch
        for batch_idx, (images, texts, text_lengths, raw_texts) in enumerate(val_loader):
            images = images.to(device)
            texts = texts.to(device)
            
            print(f"\nğŸ” Batch {batch_idx + 1} æµ‹è¯•:")
            print(f"  å›¾åƒå½¢çŠ¶: {images.shape}")
            print(f"  æ–‡æœ¬å½¢çŠ¶: {texts.shape}")
            print(f"  æ–‡æœ¬é•¿åº¦: {text_lengths}")
            print(f"  åŸå§‹æ–‡æœ¬ç¤ºä¾‹: {raw_texts[0]}")
            
            # æ£€æŸ¥ç´¢å¼•èŒƒå›´
            max_idx = texts.max().item()
            min_idx = texts.min().item()
            print(f"  æ–‡æœ¬ç´¢å¼•èŒƒå›´: [{min_idx}, {max_idx}]")
            print(f"  è¯æ±‡è¡¨èŒƒå›´: [0, {vocab_size-1}]")
            
            if max_idx >= vocab_size:
                print(f"  âŒ ç´¢å¼•è¶…å‡ºèŒƒå›´: {max_idx} >= {vocab_size}")
                return False
            else:
                print("  âœ… ç´¢å¼•èŒƒå›´æ­£å¸¸")
            
            # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
            model.eval()
            with torch.no_grad():
                try:
                    logits = model(images)
                    print(f"  æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {logits.shape}")
                    print(f"  é¢„æœŸå½¢çŠ¶: [T, B, {vocab_size}]")
                    
                    if logits.shape[2] != vocab_size:
                        print(f"  âŒ æ¨¡å‹è¾“å‡ºè¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: {logits.shape[2]} != {vocab_size}")
                        return False
                    else:
                        print("  âœ… æ¨¡å‹è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
                    
                    # æµ‹è¯•æŸå¤±è®¡ç®—
                    loss = compute_loss(logits, texts, text_lengths, blank=0)
                    print(f"  æŸå¤±å€¼: {loss.item():.4f}")
                    
                    if torch.isnan(loss) or torch.isinf(loss):
                        print("  âŒ æŸå¤±è®¡ç®—å¼‚å¸¸")
                        return False
                    else:
                        print("  âœ… æŸå¤±è®¡ç®—æ­£å¸¸")
                    
                    break
                    
                except Exception as e:
                    print(f"  âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
                    return False
            
            break
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è¯æ±‡è¡¨ä¿®å¤æˆåŠŸã€‚")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    debug_vocab_fix() 