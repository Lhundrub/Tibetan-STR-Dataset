import torch
import matplotlib.pyplot as plt
import numpy as np
from data_loader import OCRDataset
from ocr_transforms import get_ocr_transforms, get_light_augmentation, get_heavy_augmentation
import torchvision.transforms as transforms

def test_augmentation_effects():
    """æµ‹è¯•å’Œå¯è§†åŒ–æ•°æ®å¢å¼ºæ•ˆæœ"""
    print("=== æ•°æ®å¢å¼ºæ•ˆæœæµ‹è¯• ===")
    
    # ä¸åŒçš„å¢å¼ºé…ç½®
    augmentations = {
        'none': transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize((32, 128)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ]),
        'light': get_light_augmentation(),
        'medium': get_ocr_transforms(train=True),
        'heavy': get_heavy_augmentation()
    }
    
    vocab_path = 'tibetan_vocab_optimized.txt'
    
    # ä¸ºæ¯ç§å¢å¼ºåˆ›å»ºæ•°æ®é›†
    datasets = {}
    for name, transform in augmentations.items():
        try:
            dataset = OCRDataset('data/val.json', vocab_path, transform=transform)
            datasets[name] = dataset
            print(f"âœ… {name} å¢å¼ºæ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œæ ·æœ¬æ•°: {len(dataset)}")
        except Exception as e:
            print(f"âŒ {name} å¢å¼ºæ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
    
    if not datasets:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†")
        return
    
    # é€‰æ‹©åŒä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¯¹æ¯”
    sample_idx = 0
    
    print(f"\nğŸ” å¯¹æ¯”æ ·æœ¬ {sample_idx}:")
    
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    fig.suptitle('æ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”', fontsize=16)
    
    for i, (name, dataset) in enumerate(datasets.items()):
        if i >= 8:  # æœ€å¤šæ˜¾ç¤º8ä¸ª
            break
            
        try:
            # è·å–æ ·æœ¬
            img_tensor, text_idx, raw_text = dataset[sample_idx]
            
            # è½¬æ¢tensorä¸ºnumpyç”¨äºæ˜¾ç¤º
            if isinstance(img_tensor, torch.Tensor):
                img = img_tensor.squeeze().numpy()
                # åæ ‡å‡†åŒ–
                img = (img + 1) / 2
                img = np.clip(img, 0, 1)
            else:
                img = img_tensor
            
            # æ˜¾ç¤ºå›¾åƒ
            row = i // 4
            col = i % 4
            axes[row, col].imshow(img, cmap='gray')
            axes[row, col].set_title(f'{name.capitalize()}')
            axes[row, col].axis('off')
            
            print(f"  {name}: æ–‡æœ¬='{raw_text}', å½¢çŠ¶={img.shape}")
            
        except Exception as e:
            print(f"  âŒ {name} å¤„ç†å¤±è´¥: {e}")
            axes[i//4, i%4].text(0.5, 0.5, f'Error\n{name}', 
                                ha='center', va='center', transform=axes[i//4, i%4].transAxes)
            axes[i//4, i%4].axis('off')
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(len(datasets), 8):
        axes[i//4, i%4].axis('off')
    
    plt.tight_layout()
    plt.savefig('augmentation_comparison.png', dpi=150, bbox_inches='tight')
    print(f"\nğŸ“¸ å¢å¼ºå¯¹æ¯”å›¾å·²ä¿å­˜: augmentation_comparison.png")

def test_multiple_samples():
    """æµ‹è¯•åŒä¸€ç§å¢å¼ºçš„å¤šä¸ªæ ·æœ¬"""
    print("\n=== ä¸­ç­‰å¼ºåº¦å¢å¼ºå¤šæ ·æœ¬æµ‹è¯• ===")
    
    vocab_path = 'tibetan_vocab_optimized.txt'
    transform = get_ocr_transforms(train=True)
    
    try:
        dataset = OCRDataset('data/val.json', vocab_path, transform=transform)
        
        # é€‰æ‹©å‰6ä¸ªæ ·æœ¬
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle('ä¸­ç­‰å¼ºåº¦æ•°æ®å¢å¼ºæ ·æœ¬å±•ç¤º', fontsize=16)
        
        for i in range(6):
            if i >= len(dataset):
                break
                
            img_tensor, text_idx, raw_text = dataset[i]
            
            # è½¬æ¢æ˜¾ç¤º
            if isinstance(img_tensor, torch.Tensor):
                img = img_tensor.squeeze().numpy()
                img = (img + 1) / 2
                img = np.clip(img, 0, 1)
            else:
                img = img_tensor
            
            row = i // 3
            col = i % 3
            axes[row, col].imshow(img, cmap='gray')
            axes[row, col].set_title(f"'{raw_text}'", fontsize=10)
            axes[row, col].axis('off')
            
            print(f"  æ ·æœ¬ {i+1}: '{raw_text}'")
        
        plt.tight_layout()
        plt.savefig('medium_augmentation_samples.png', dpi=150, bbox_inches='tight')
        print(f"ğŸ“¸ å¤šæ ·æœ¬å±•ç¤ºå·²ä¿å­˜: medium_augmentation_samples.png")
        
    except Exception as e:
        print(f"âŒ å¤šæ ·æœ¬æµ‹è¯•å¤±è´¥: {e}")

def analyze_augmentation_impact():
    """åˆ†ææ•°æ®å¢å¼ºå¯¹è®­ç»ƒçš„æ½œåœ¨å½±å“"""
    print("\n=== æ•°æ®å¢å¼ºå½±å“åˆ†æ ===")
    
    vocab_path = 'tibetan_vocab_optimized.txt'
    
    # æµ‹è¯•ä¸åŒå¢å¼ºçš„batch
    augmentations = {
        'none': transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize((32, 128)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ]),
        'medium': get_ocr_transforms(train=True)
    }
    
    from torch.utils.data import DataLoader
    from data_loader import collate_fn
    
    for name, transform in augmentations.items():
        print(f"\nğŸ“Š {name.capitalize()} å¢å¼ºåˆ†æ:")
        
        try:
            dataset = OCRDataset('data/val.json', vocab_path, transform=transform)
            loader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)
            
            # è·å–ä¸€ä¸ªbatch
            for images, texts, text_lengths, raw_texts in loader:
                print(f"  Batchå½¢çŠ¶: {images.shape}")
                print(f"  å›¾åƒå€¼èŒƒå›´: [{images.min().item():.3f}, {images.max().item():.3f}]")
                print(f"  å›¾åƒå‡å€¼: {images.mean().item():.3f}")
                print(f"  å›¾åƒæ ‡å‡†å·®: {images.std().item():.3f}")
                print(f"  æ–‡æœ¬ç¤ºä¾‹: {raw_texts[0]}")
                break
                
        except Exception as e:
            print(f"  âŒ åˆ†æå¤±è´¥: {e}")
    
    print("\nğŸ’¡ å»ºè®®:")
    print("  - none: é€‚ç”¨äºé«˜è´¨é‡ã€ä¸€è‡´çš„æ•°æ®")
    print("  - light: é€‚ç”¨äºæ•°æ®é‡è¾ƒå¤§çš„æƒ…å†µ")
    print("  - medium: å¹³è¡¡çš„é€‰æ‹©ï¼Œé€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µ")
    print("  - heavy: é€‚ç”¨äºæ•°æ®é‡å°æˆ–è´¨é‡ä¸ä¸€çš„æƒ…å†µ")

if __name__ == "__main__":
    test_augmentation_effects()
    test_multiple_samples()
    analyze_augmentation_impact() 