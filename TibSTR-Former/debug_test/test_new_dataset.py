import os
import sys
import json
import torch
from torch.utils.data import DataLoader
from torchvision import transforms

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data_loader import OCRDataset, collate_fn


def test_dataset_compatibility():
    print("=== æµ‹è¯•æ–°æ•°æ®é›†å…¼å®¹æ€§ ===")
    
    train_path = 'data/train_new.json'
    val_path = 'data/val_new.json'
    vocab_path = 'tibetan_vocab_full.txt'
    
    for path in [train_path, val_path, vocab_path]:
        if not os.path.exists(path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return False
        else:
            print(f"âœ… æ–‡ä»¶å­˜åœ¨: {path}")
    
    try:
        transform = transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize((32, 128)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])
        
        print("\n=== æµ‹è¯•æ•°æ®é›†åŠ è½½ ===")
        
        print("æ­£åœ¨åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
        train_dataset = OCRDataset(train_path, vocab_path, transform=transform)
        print(f"âœ… è®­ç»ƒæ•°æ®é›†åˆ›å»ºæˆåŠŸ: {len(train_dataset)} æ ·æœ¬")
        
        print("æ­£åœ¨åˆ›å»ºéªŒè¯æ•°æ®é›†...")
        val_dataset = OCRDataset(val_path, vocab_path, transform=transform)
        print(f"âœ… éªŒè¯æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {len(val_dataset)} æ ·æœ¬")
        
        print("\n=== æµ‹è¯•DataLoader ===")
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=4, 
            shuffle=True, 
            collate_fn=collate_fn
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=4, 
            shuffle=False, 
            collate_fn=collate_fn
        )
        
        print(f"âœ… DataLoaderåˆ›å»ºæˆåŠŸ")
        print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
        
        print("\n=== æµ‹è¯•æ•°æ®æ‰¹æ¬¡ ===")
        
        for i, (images, texts, text_lengths, raw_texts) in enumerate(train_loader):
            print(f"æ‰¹æ¬¡ {i+1}:")
            print(f"  å›¾åƒå½¢çŠ¶: {images.shape}")
            print(f"  æ–‡æœ¬å½¢çŠ¶: {texts.shape}")
            print(f"  æ–‡æœ¬é•¿åº¦: {text_lengths}")
            print(f"  åŸå§‹æ–‡æœ¬æ•°é‡: {len(raw_texts)}")
            print(f"  åŸå§‹æ–‡æœ¬ç¤ºä¾‹: {raw_texts[:2]}")
            
            assert isinstance(images, torch.Tensor), "å›¾åƒåº”è¯¥æ˜¯tensorç±»å‹"
            assert isinstance(texts, torch.Tensor), "æ–‡æœ¬åº”è¯¥æ˜¯tensorç±»å‹"
            assert isinstance(text_lengths, list), "æ–‡æœ¬é•¿åº¦åº”è¯¥æ˜¯åˆ—è¡¨ç±»å‹"
            assert isinstance(raw_texts, list), "åŸå§‹æ–‡æœ¬åº”è¯¥æ˜¯åˆ—è¡¨ç±»å‹"
            
            if i >= 2:
                break
        
        print("âœ… æ•°æ®æ‰¹æ¬¡æµ‹è¯•é€šè¿‡")
        
        print("\n=== æµ‹è¯•è¯æ±‡è¡¨å…¼å®¹æ€§ ===")
        
        with open(vocab_path, 'r', encoding='utf-8') as f:
            vocab = [line.strip() for line in f if line.strip()]
        char2idx = {char: idx for idx, char in enumerate(vocab)}
        idx2char = {idx: char for idx, char in enumerate(vocab)}
        
        print(f"è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        
        sample_texts = raw_texts[:3]
        encoded_successfully = 0
        
        for text in sample_texts:
            try:
                encoded = [char2idx.get(char, char2idx.get('<unk>', 0)) for char in text]
                decoded = ''.join([idx2char.get(idx, '') for idx in encoded])
                print(f"  åŸæ–‡: '{text}'")
                print(f"  ç¼–ç : {encoded[:10]}{'...' if len(encoded) > 10 else ''}")
                print(f"  è§£ç : '{decoded}'")
                encoded_successfully += 1
            except Exception as e:
                print(f"  âŒ ç¼–ç å¤±è´¥: {text} - {str(e)}")
        
        print(f"âœ… æˆåŠŸç¼–ç  {encoded_successfully}/{len(sample_texts)} ä¸ªæ ·æœ¬")
        
        print("\n=== æ€§èƒ½æµ‹è¯• ===")
        
        import time
        start_time = time.time()
        batch_count = 0
        
        for batch in train_loader:
            batch_count += 1
            if batch_count >= 10:
                break
        
        elapsed_time = time.time() - start_time
        print(f"åŠ è½½10ä¸ªæ‰¹æ¬¡ç”¨æ—¶: {elapsed_time:.2f}ç§’")
        print(f"å¹³å‡æ¯æ‰¹æ¬¡: {elapsed_time/batch_count:.2f}ç§’")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def compare_old_new_datasets():
    print("\n=== æ¯”è¾ƒæ—§æ•°æ®é›†å’Œæ–°æ•°æ®é›† ===")
    
    try:
        old_train_path = 'data/train.json'
        old_val_path = 'data/val.json'
        new_train_path = 'data/train_new.json'
        new_val_path = 'data/val_new.json'
        
        datasets = {
            'old_train': old_train_path,
            'old_val': old_val_path,
            'new_train': new_train_path,
            'new_val': new_val_path
        }
        
        data_stats = {}
        
        for name, path in datasets.items():
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                total_samples = len(data)
                if data:
                    sample = data[0]
                    data_format = list(sample.keys())
                    
                    if 'text' in sample:
                        text_lengths = [len(item['text']) for item in data]
                    elif 'transcription' in sample:
                        text_lengths = []
                        for item in data:
                            if isinstance(item['transcription'], list):
                                text_lengths.extend([len(text) for text in item['transcription']])
                            else:
                                text_lengths.append(len(item['transcription']))
                    else:
                        text_lengths = []
                    
                    stats = {
                        'total_samples': total_samples,
                        'data_format': data_format,
                        'avg_text_length': sum(text_lengths) / len(text_lengths) if text_lengths else 0,
                        'min_text_length': min(text_lengths) if text_lengths else 0,
                        'max_text_length': max(text_lengths) if text_lengths else 0,
                    }
                    
                    data_stats[name] = stats
                    print(f"\n{name}:")
                    print(f"  æ ·æœ¬æ•°: {stats['total_samples']}")
                    print(f"  æ•°æ®æ ¼å¼: {stats['data_format']}")
                    print(f"  å¹³å‡æ–‡æœ¬é•¿åº¦: {stats['avg_text_length']:.2f}")
                    print(f"  æ–‡æœ¬é•¿åº¦èŒƒå›´: {stats['min_text_length']} - {stats['max_text_length']}")
                else:
                    print(f"{name}: ç©ºæ•°æ®é›†")
            else:
                print(f"{name}: æ–‡ä»¶ä¸å­˜åœ¨ ({path})")
        
        return data_stats
        
    except Exception as e:
        print(f"âŒ æ¯”è¾ƒå¤±è´¥: {str(e)}")
        return None


def test_with_existing_training_code():
    print("\n=== æµ‹è¯•ç°æœ‰è®­ç»ƒä»£ç å…¼å®¹æ€§ ===")
    
    try:
        from model import CNNTransformer
        from train import compute_loss, decode_predictions
        
        vocab_path = 'tibetan_vocab_full.txt'
        with open(vocab_path, 'r', encoding='utf-8') as f:
            vocab = [line.strip() for line in f if line.strip()]
        vocab_size = len(vocab)
        
        model = CNNTransformer(vocab_size)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {vocab_size}")
        
        transform = transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize((32, 128)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])
        
        dataset = OCRDataset('data/train_new.json', vocab_path, transform=transform)
        dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
        
        print("âœ… æ•°æ®é›†å’ŒDataLoaderåˆ›å»ºæˆåŠŸ")
        
        model.eval()
        char2idx = {char: idx for idx, char in enumerate(vocab)}
        idx2char = {idx: char for idx, char in enumerate(vocab)}
        
        for images, texts, text_lengths, raw_texts in dataloader:
            images = images.to(device)
            texts = texts.to(device)
            
            print(f"è¾“å…¥å›¾åƒå½¢çŠ¶: {images.shape}")
            print(f"è¾“å…¥æ–‡æœ¬å½¢çŠ¶: {texts.shape}")
            
            with torch.no_grad():
                logits = model(images)
                print(f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {logits.shape}")
                
                loss = compute_loss(logits, texts, text_lengths, blank=0)
                print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸ: {loss.item():.4f}")
                
                predictions = decode_predictions(logits, idx2char, blank=0)
                print(f"âœ… è§£ç æˆåŠŸï¼Œé¢„æµ‹ç»“æœ: {predictions}")
                print(f"çœŸå®æ–‡æœ¬: {raw_texts}")
            
            break
        
        print("âœ… ç°æœ‰è®­ç»ƒä»£ç å…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def main():
    print("=== æ–°æ•°æ®é›†å®Œæ•´æµ‹è¯• ===")
    print("æµ‹è¯•æ–°åˆ’åˆ†çš„æ•°æ®é›†æ˜¯å¦èƒ½æ­£ç¡®åŠ è½½å’Œä½¿ç”¨")
    print("=" * 50)
    
    success_count = 0
    total_tests = 3
    
    if test_dataset_compatibility():
        success_count += 1
        print("âœ… æµ‹è¯•1é€šè¿‡: æ•°æ®é›†å…¼å®¹æ€§")
    else:
        print("âŒ æµ‹è¯•1å¤±è´¥: æ•°æ®é›†å…¼å®¹æ€§")
    
    stats = compare_old_new_datasets()
    if stats:
        success_count += 1
        print("âœ… æµ‹è¯•2é€šè¿‡: æ•°æ®é›†æ¯”è¾ƒ")
    else:
        print("âŒ æµ‹è¯•2å¤±è´¥: æ•°æ®é›†æ¯”è¾ƒ")
    
    if test_with_existing_training_code():
        success_count += 1
        print("âœ… æµ‹è¯•3é€šè¿‡: è®­ç»ƒä»£ç å…¼å®¹æ€§")
    else:
        print("âŒ æµ‹è¯•3å¤±è´¥: è®­ç»ƒä»£ç å…¼å®¹æ€§")
    
    print("\n" + "=" * 50)
    print(f"æµ‹è¯•ç»“æœ: {success_count}/{total_tests} é€šè¿‡")
    
    if success_count == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–°æ•°æ®é›†å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é—®é¢˜ã€‚")
        return False


if __name__ == '__main__':
    main() 